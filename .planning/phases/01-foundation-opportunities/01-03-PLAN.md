---
phase: 01-foundation-opportunities
plan: 03
type: execute
wave: 3
depends_on: ['01-02']
files_modified:
  - convex/aggregation/eightyK.ts
  - convex/aggregation/aisafety.ts
  - convex/aggregation/dedup.ts
  - convex/aggregation/sync.ts
  - convex/crons.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - 'Running sync action imports opportunities from 80K Hours'
    - 'Running sync action imports opportunities from aisafety.com Airtable'
    - 'Duplicate opportunities from different sources are merged'
    - 'Opportunities no longer in source are auto-archived'
    - 'Cron job scheduled for daily execution'
  artifacts:
    - path: 'convex/aggregation/eightyK.ts'
      provides: '80K Hours Algolia adapter'
      exports: ['fetchOpportunities']
    - path: 'convex/aggregation/aisafety.ts'
      provides: 'aisafety.com Airtable adapter'
      exports: ['fetchOpportunities']
    - path: 'convex/aggregation/sync.ts'
      provides: 'Sync orchestration'
      exports: ['runFullSync', 'upsertOpportunities']
    - path: 'convex/crons.ts'
      provides: 'Scheduled job definitions'
      min_lines: 10
  key_links:
    - from: 'convex/crons.ts'
      to: 'convex/aggregation/sync.ts'
      via: 'crons.daily'
      pattern: "internal\\.aggregation\\.sync\\.runFullSync"
    - from: 'convex/aggregation/sync.ts'
      to: 'convex/aggregation/eightyK.ts'
      via: 'ctx.runAction'
      pattern: "internal\\.aggregation\\.eightyK"
---

<objective>
Build opportunity aggregation from 80K Hours (via Algolia API) and aisafety.com (via Airtable API), with duplicate detection and scheduled daily sync.

Purpose: Automatically populate opportunities database from external sources (OPPS-03, OPPS-04), preventing cold start.

Output: Working adapters that import opportunities on schedule, with duplicates merged and stale listings archived.

Note: This plan is entirely backend/Convex code. All functions run in the Convex runtime and are framework-agnostic - no Next.js or TanStack Start patterns are involved. The code works identically regardless of which frontend framework is used.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation-opportunities/CONTEXT.md
@.planning/phases/01-foundation-opportunities/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create 80K Hours Algolia adapter</name>
  <files>
    convex/aggregation/eightyK.ts
    package.json
  </files>
  <action>
1. Install Algolia client:
```bash
bun add algoliasearch
```

2. Create `convex/aggregation/eightyK.ts`:

```typescript
'use node'
import { action } from '../_generated/server'
import algoliasearch from 'algoliasearch'

// 80K Hours uses Algolia for their job board search
// Credentials discovered from page source (public frontend keys)
// These may need updating if 80K Hours rotates their keys
const ALGOLIA_APP_ID = process.env.EIGHTY_K_ALGOLIA_APP_ID || ''
const ALGOLIA_API_KEY = process.env.EIGHTY_K_ALGOLIA_API_KEY || ''
const ALGOLIA_INDEX = 'jobs_prod_super_ranked'

type AlgoliaHit = {
  objectID: string
  title: string
  company_name: string
  location?: string
  remote?: boolean
  job_type?: string
  experience_required?: string
  description_short?: string
  description?: string
  requirements?: string[]
  salary_text?: string
  closing_date?: string
  posted_date?: string
  url: string
}

type NormalizedOpportunity = {
  sourceId: string
  source: '80k_hours'
  title: string
  organization: string
  location: string
  isRemote: boolean
  roleType: string
  experienceLevel?: string
  description: string
  requirements?: string[]
  salaryRange?: string
  deadline?: number
  sourceUrl: string
  postedAt?: number
}

export const fetchOpportunities = action({
  args: {},
  handler: async (): Promise<NormalizedOpportunity[]> => {
    if (!ALGOLIA_APP_ID || !ALGOLIA_API_KEY) {
      console.error('Missing 80K Hours Algolia credentials')
      return []
    }

    const client = algoliasearch(ALGOLIA_APP_ID, ALGOLIA_API_KEY)

    const results: AlgoliaHit[] = []
    let page = 0
    let hasMore = true

    try {
      while (hasMore) {
        const response = await client.searchSingleIndex({
          indexName: ALGOLIA_INDEX,
          searchParams: {
            query: '',
            page,
            hitsPerPage: 100,
          },
        })

        results.push(...(response.hits as AlgoliaHit[]))
        hasMore = page < (response.nbPages ?? 1) - 1
        page++

        // Rate limiting: 1 second between requests
        if (hasMore) {
          await new Promise((resolve) => setTimeout(resolve, 1000))
        }
      }

      console.log(`Fetched ${results.length} opportunities from 80K Hours`)
      return results.map(normalizeEightyKJob)
    } catch (error) {
      console.error('Error fetching from 80K Hours:', error)
      return []
    }
  },
})

function normalizeEightyKJob(hit: AlgoliaHit): NormalizedOpportunity {
  return {
    sourceId: `80k-${hit.objectID}`,
    source: '80k_hours',
    title: hit.title || 'Untitled',
    organization: hit.company_name || 'Unknown Organization',
    location: hit.location || 'Location not specified',
    isRemote:
      hit.remote ?? hit.location?.toLowerCase().includes('remote') ?? false,
    roleType: mapRoleType(hit.job_type),
    experienceLevel: mapExperienceLevel(hit.experience_required),
    description: hit.description || hit.description_short || '',
    requirements: hit.requirements,
    salaryRange: hit.salary_text,
    deadline: hit.closing_date
      ? new Date(hit.closing_date).getTime()
      : undefined,
    sourceUrl: hit.url,
    postedAt: hit.posted_date ? new Date(hit.posted_date).getTime() : undefined,
  }
}

function mapRoleType(jobType?: string): string {
  if (!jobType) return 'other'
  const lower = jobType.toLowerCase()
  if (lower.includes('research')) return 'research'
  if (
    lower.includes('engineer') ||
    lower.includes('software') ||
    lower.includes('technical')
  )
    return 'engineering'
  if (
    lower.includes('operations') ||
    lower.includes('ops') ||
    lower.includes('admin')
  )
    return 'operations'
  if (lower.includes('policy') || lower.includes('governance')) return 'policy'
  return 'other'
}

function mapExperienceLevel(exp?: string): string | undefined {
  if (!exp) return undefined
  const lower = exp.toLowerCase()
  if (
    lower.includes('entry') ||
    lower.includes('junior') ||
    lower.includes('0-2')
  )
    return 'entry'
  if (lower.includes('mid') || lower.includes('2-5') || lower.includes('3-5'))
    return 'mid'
  if (
    lower.includes('senior') ||
    lower.includes('5+') ||
    lower.includes('5-10')
  )
    return 'senior'
  if (
    lower.includes('lead') ||
    lower.includes('principal') ||
    lower.includes('director')
  )
    return 'lead'
  return undefined
}
```

3. Add environment variables to `.env.local`:

```
# 80K Hours Algolia (extract from 80000hours.org/jobs page source)
EIGHTY_K_ALGOLIA_APP_ID=
EIGHTY_K_ALGOLIA_API_KEY=
```

NOTE: The Algolia credentials need to be extracted from the 80K Hours website page source. Visit https://80000hours.org/job-board/ and view source to find the Algolia config. These are public frontend keys.
</action>
<verify>
File exists: `convex/aggregation/eightyK.ts`
`bunx convex dev` shows function deployed without errors.
</verify>
<done>80K Hours adapter ready (will need credentials populated to actually fetch data).</done>
</task>

<task type="auto">
  <name>Task 2: Create aisafety.com Airtable adapter</name>
  <files>
    convex/aggregation/aisafety.ts
    package.json
  </files>
  <action>
The aisafety.com team has provided Airtable API access. This is much simpler and more reliable than Playwright scraping.

1. No additional dependencies needed - we'll use fetch with the Airtable REST API.

2. Create `convex/aggregation/aisafety.ts`:

```typescript
'use node'
import { action } from '../_generated/server'
import crypto from 'crypto'

// Airtable API access provided by aisafety.com team
const AIRTABLE_API_KEY = process.env.AISAFETY_AIRTABLE_API_KEY || ''
const AIRTABLE_BASE_ID = process.env.AISAFETY_AIRTABLE_BASE_ID || ''
const AIRTABLE_TABLE_NAME = process.env.AISAFETY_AIRTABLE_TABLE_NAME || 'Jobs'

type AirtableRecord = {
  id: string
  fields: {
    Title?: string
    Organization?: string
    Location?: string
    Remote?: boolean
    Description?: string
    Requirements?: string
    'Application URL'?: string
    'Role Type'?: string
    Salary?: string
    Deadline?: string
  }
}

type NormalizedOpportunity = {
  sourceId: string
  source: 'aisafety_com'
  title: string
  organization: string
  location: string
  isRemote: boolean
  roleType: string
  experienceLevel?: string
  description: string
  requirements?: string[]
  salaryRange?: string
  deadline?: number
  sourceUrl: string
}

export const fetchOpportunities = action({
  args: {},
  handler: async (): Promise<NormalizedOpportunity[]> => {
    if (!AIRTABLE_API_KEY || !AIRTABLE_BASE_ID) {
      console.error('Missing aisafety.com Airtable credentials')
      return []
    }

    const results: AirtableRecord[] = []
    let offset: string | undefined

    try {
      // Paginate through all records
      do {
        const url = new URL(
          `https://api.airtable.com/v0/${AIRTABLE_BASE_ID}/${encodeURIComponent(AIRTABLE_TABLE_NAME)}`,
        )
        if (offset) {
          url.searchParams.set('offset', offset)
        }
        url.searchParams.set('pageSize', '100')

        const response = await fetch(url.toString(), {
          headers: {
            Authorization: `Bearer ${AIRTABLE_API_KEY}`,
            'Content-Type': 'application/json',
          },
        })

        if (!response.ok) {
          throw new Error(
            `Airtable API error: ${response.status} ${response.statusText}`,
          )
        }

        const data = await response.json()
        results.push(...(data.records as AirtableRecord[]))
        offset = data.offset

        // Rate limiting: Airtable allows 5 requests/second
        if (offset) {
          await new Promise((resolve) => setTimeout(resolve, 250))
        }
      } while (offset)

      console.log(
        `Fetched ${results.length} opportunities from aisafety.com Airtable`,
      )

      return results
        .filter((record) => record.fields.Title)
        .map((record) => normalizeAirtableRecord(record))
    } catch (error) {
      console.error('Error fetching from aisafety.com Airtable:', error)
      return []
    }
  },
})

function normalizeAirtableRecord(
  record: AirtableRecord,
): NormalizedOpportunity {
  const fields = record.fields

  return {
    sourceId: `aisafety-${record.id}`,
    source: 'aisafety_com',
    title: fields.Title || 'Untitled',
    organization: fields.Organization || 'Unknown',
    location: fields.Location || 'Remote',
    isRemote:
      fields.Remote ??
      fields.Location?.toLowerCase().includes('remote') ??
      false,
    roleType: mapRoleType(fields['Role Type'] || fields.Title || ''),
    description: fields.Description || '',
    requirements: fields.Requirements
      ? fields.Requirements.split('\n').filter(Boolean)
      : undefined,
    salaryRange: fields.Salary,
    deadline: fields.Deadline ? new Date(fields.Deadline).getTime() : undefined,
    sourceUrl: fields['Application URL'] || `https://www.aisafety.com/jobs`,
  }
}

function mapRoleType(input: string): string {
  const lower = input.toLowerCase()
  if (lower.includes('research')) return 'research'
  if (
    lower.includes('engineer') ||
    lower.includes('developer') ||
    lower.includes('software')
  )
    return 'engineering'
  if (
    lower.includes('operations') ||
    lower.includes('ops') ||
    lower.includes('coordinator')
  )
    return 'operations'
  if (lower.includes('policy') || lower.includes('governance')) return 'policy'
  return 'other'
}
```

3. Add environment variables to `.env.local` and Convex dashboard:

```
# aisafety.com Airtable API (provided by their team)
AISAFETY_AIRTABLE_API_KEY=
AISAFETY_AIRTABLE_BASE_ID=
AISAFETY_AIRTABLE_TABLE_NAME=Jobs
```

NOTE: The Airtable credentials will be provided by the aisafety.com team. Field names may need adjustment based on their actual schema.
</action>
<verify>
File exists: `convex/aggregation/aisafety.ts`
`bunx convex dev` shows function deployed without errors.
</verify>
<done>aisafety.com Airtable adapter ready (will need credentials from their team).</done>
</task>

<task type="auto">
  <name>Task 3: Create sync orchestration with deduplication</name>
  <files>
    convex/aggregation/dedup.ts
    convex/aggregation/sync.ts
    package.json
  </files>
  <action>
1. Install string-similarity-js (maintained fork of deprecated string-similarity):
```bash
bun add string-similarity-js
```

2. Create `convex/aggregation/dedup.ts`:

```typescript
import { stringSimilarity } from 'string-similarity-js'

export function normalizeTitle(title: string): string {
  return title
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, '')
    .replace(/\s+/g, ' ')
    .trim()
}

export function normalizeOrganization(org: string): string {
  return org
    .toLowerCase()
    .replace(/,?\s*(inc|llc|ltd|pbc|corp|corporation)\.?$/i, '')
    .replace(/[^a-z0-9\s]/g, '')
    .replace(/\s+/g, ' ')
    .trim()
}

export function isSimilarOpportunity(
  a: { title: string; organization: string },
  b: { title: string; organization: string },
  threshold = 0.85,
): boolean {
  const titleSimilarity = stringSimilarity(
    normalizeTitle(a.title),
    normalizeTitle(b.title),
  )

  const orgSimilarity = stringSimilarity(
    normalizeOrganization(a.organization),
    normalizeOrganization(b.organization),
  )

  // Both title and org must be similar
  return titleSimilarity > threshold && orgSimilarity > 0.8
}
```

3. Create `convex/aggregation/sync.ts`:

```typescript
'use node'
import { internalAction, internalMutation } from '../_generated/server'
import { internal } from '../_generated/api'
import { v } from 'convex/values'
import { isSimilarOpportunity } from './dedup'

const opportunityValidator = v.object({
  sourceId: v.string(),
  source: v.union(
    v.literal('80k_hours'),
    v.literal('aisafety_com'),
    v.literal('manual'),
  ),
  title: v.string(),
  organization: v.string(),
  location: v.string(),
  isRemote: v.boolean(),
  roleType: v.string(),
  experienceLevel: v.optional(v.string()),
  description: v.string(),
  requirements: v.optional(v.array(v.string())),
  salaryRange: v.optional(v.string()),
  deadline: v.optional(v.number()),
  sourceUrl: v.string(),
  postedAt: v.optional(v.number()),
})

export const runFullSync = internalAction({
  args: {},
  handler: async (ctx) => {
    console.log('Starting full opportunity sync...')

    // Fetch from both sources in parallel
    const [eightyKJobs, aisafetyJobs] = await Promise.all([
      ctx.runAction(internal.aggregation.eightyK.fetchOpportunities, {}),
      ctx.runAction(internal.aggregation.aisafety.fetchOpportunities, {}),
    ])

    console.log(
      `Fetched: ${eightyKJobs.length} from 80K Hours, ${aisafetyJobs.length} from aisafety.com`,
    )

    // Combine all jobs
    const allJobs = [...eightyKJobs, ...aisafetyJobs]

    if (allJobs.length === 0) {
      console.log('No opportunities fetched from any source')
      return
    }

    // Upsert opportunities with deduplication
    await ctx.runMutation(internal.aggregation.sync.upsertOpportunities, {
      opportunities: allJobs,
    })

    // Archive opportunities that disappeared from sources
    await ctx.runMutation(internal.aggregation.sync.archiveMissing, {
      currentSourceIds: allJobs.map((j) => j.sourceId),
    })

    console.log('Sync complete')
  },
})

export const upsertOpportunities = internalMutation({
  args: {
    opportunities: v.array(opportunityValidator),
  },
  handler: async (ctx, args) => {
    let inserted = 0
    let updated = 0
    let merged = 0

    for (const opp of args.opportunities) {
      // Check for exact source match first
      const existing = await ctx.db
        .query('opportunities')
        .withIndex('by_source_id', (q) => q.eq('sourceId', opp.sourceId))
        .unique()

      if (existing) {
        // Update existing opportunity
        await ctx.db.patch(existing._id, {
          title: opp.title,
          organization: opp.organization,
          location: opp.location,
          isRemote: opp.isRemote,
          roleType: opp.roleType,
          experienceLevel: opp.experienceLevel,
          description: opp.description,
          requirements: opp.requirements,
          salaryRange: opp.salaryRange,
          deadline: opp.deadline,
          sourceUrl: opp.sourceUrl,
          lastVerified: Date.now(),
          updatedAt: Date.now(),
        })
        updated++
      } else {
        // Check for fuzzy duplicate from different source
        const sameOrg = await ctx.db
          .query('opportunities')
          .withIndex('by_organization', (q) =>
            q.eq('organization', opp.organization),
          )
          .collect()

        const duplicate = sameOrg.find((existing) =>
          isSimilarOpportunity(
            { title: existing.title, organization: existing.organization },
            { title: opp.title, organization: opp.organization },
          ),
        )

        if (duplicate) {
          // Add as alternate source
          const alternateSources = duplicate.alternateSources || []
          const alreadyListed = alternateSources.some(
            (s) => s.sourceId === opp.sourceId,
          )

          if (!alreadyListed) {
            await ctx.db.patch(duplicate._id, {
              alternateSources: [
                ...alternateSources,
                {
                  sourceId: opp.sourceId,
                  source: opp.source,
                  sourceUrl: opp.sourceUrl,
                },
              ],
              lastVerified: Date.now(),
              updatedAt: Date.now(),
            })
            merged++
          }
        } else {
          // Insert new opportunity
          await ctx.db.insert('opportunities', {
            sourceId: opp.sourceId,
            source: opp.source,
            title: opp.title,
            organization: opp.organization,
            location: opp.location,
            isRemote: opp.isRemote,
            roleType: opp.roleType,
            experienceLevel: opp.experienceLevel,
            description: opp.description,
            requirements: opp.requirements,
            salaryRange: opp.salaryRange,
            deadline: opp.deadline,
            sourceUrl: opp.sourceUrl,
            status: 'active',
            lastVerified: Date.now(),
            createdAt: Date.now(),
            updatedAt: Date.now(),
          })
          inserted++
        }
      }
    }

    console.log(
      `Upsert complete: ${inserted} inserted, ${updated} updated, ${merged} merged`,
    )
  },
})

export const archiveMissing = internalMutation({
  args: {
    currentSourceIds: v.array(v.string()),
  },
  handler: async (ctx, args) => {
    const sourceIdSet = new Set(args.currentSourceIds)

    // Get all active non-manual opportunities
    const activeOpportunities = await ctx.db
      .query('opportunities')
      .withIndex('by_status', (q) => q.eq('status', 'active'))
      .collect()

    let archived = 0
    for (const opp of activeOpportunities) {
      // Don't archive manual entries or opportunities still in source
      if (opp.source === 'manual' || sourceIdSet.has(opp.sourceId)) {
        continue
      }

      // Also check if any alternate source is still active
      const hasActiveAlternate = opp.alternateSources?.some((alt) =>
        sourceIdSet.has(alt.sourceId),
      )

      if (!hasActiveAlternate) {
        await ctx.db.patch(opp._id, {
          status: 'archived',
          updatedAt: Date.now(),
        })
        archived++
      }
    }

    if (archived > 0) {
      console.log(`Archived ${archived} opportunities no longer in sources`)
    }
  },
})

// Manual trigger for testing
export const triggerSync = internalAction({
  args: {},
  handler: async (ctx) => {
    await ctx.runAction(internal.aggregation.sync.runFullSync, {})
  },
})
```

  </action>
  <verify>
Files exist: `convex/aggregation/dedup.ts`, `convex/aggregation/sync.ts`
`bunx convex dev` shows all functions deployed without errors.
  </verify>
  <done>Sync orchestration with deduplication and archiving logic complete.</done>
</task>

<task type="auto">
  <name>Task 4: Set up cron job for daily sync</name>
  <files>
    convex/crons.ts
  </files>
  <action>
Create `convex/crons.ts`:

```typescript
import { cronJobs } from 'convex/server'
import { internal } from './_generated/api'

const crons = cronJobs()

// Run daily at 6 AM UTC
// This syncs opportunities from 80K Hours and aisafety.com
crons.daily(
  'sync-opportunities',
  { hourUTC: 6, minuteUTC: 0 },
  internal.aggregation.sync.runFullSync,
)

export default crons
```

After creating the file, verify cron is registered:

- Run `bunx convex dev`
- Check Convex dashboard -> Functions -> Crons tab
- Should show "sync-opportunities" scheduled for 6:00 UTC daily

To manually trigger sync for testing:

1. Go to Convex dashboard
2. Functions tab
3. Find `aggregation.sync.triggerSync`
4. Click "Run" to execute manually

NOTE: For first run, you'll need to populate the 80K Hours Algolia credentials (from page source) and aisafety.com Airtable credentials (from their team).
</action>
<verify>
File exists: `convex/crons.ts`
Convex dashboard shows cron job registered.
</verify>
<done>Daily sync cron configured at 6 AM UTC.</done>
</task>

</tasks>

<verification>
Verify plan completion:

```bash
# All files exist
ls convex/aggregation/

# Convex shows functions
bunx convex dev

# Check dashboard:
# - Functions tab shows eightyK.fetchOpportunities, aisafety.fetchOpportunities, sync.runFullSync
# - Crons tab shows sync-opportunities scheduled
```

Manual test (after adding credentials):

1. Go to Convex dashboard
2. Run `aggregation.sync.triggerSync` action
3. Check Data tab - opportunities should appear
4. Run again - should update existing, not duplicate
   </verification>

<success_criteria>

- 80K Hours adapter queries Algolia API (OPPS-03)
- aisafety.com adapter fetches from Airtable API (OPPS-04)
- Duplicate opportunities from different sources are merged
- Opportunities removed from source are auto-archived
- Cron job runs daily at 6 AM UTC
- lastVerified updated on each sync (supports OPPS-06)
  </success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-opportunities/01-03-SUMMARY.md`
</output>
