---
phase: 01-foundation-opportunities
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - convex/aggregation/eightyK.ts
  - convex/aggregation/aisafety.ts
  - convex/aggregation/dedup.ts
  - convex/aggregation/sync.ts
  - convex/crons.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Running sync action imports opportunities from 80K Hours"
    - "Running sync action imports opportunities from aisafety.com"
    - "Duplicate opportunities from different sources are merged"
    - "Opportunities no longer in source are auto-archived"
    - "Cron job scheduled for daily execution"
  artifacts:
    - path: "convex/aggregation/eightyK.ts"
      provides: "80K Hours Algolia adapter"
      exports: ["fetchOpportunities"]
    - path: "convex/aggregation/aisafety.ts"
      provides: "aisafety.com Playwright scraper"
      exports: ["fetchOpportunities"]
    - path: "convex/aggregation/sync.ts"
      provides: "Sync orchestration"
      exports: ["runFullSync", "upsertOpportunities"]
    - path: "convex/crons.ts"
      provides: "Scheduled job definitions"
      min_lines: 10
  key_links:
    - from: "convex/crons.ts"
      to: "convex/aggregation/sync.ts"
      via: "crons.daily"
      pattern: "internal\\.aggregation\\.sync\\.runFullSync"
    - from: "convex/aggregation/sync.ts"
      to: "convex/aggregation/eightyK.ts"
      via: "ctx.runAction"
      pattern: "internal\\.aggregation\\.eightyK"
---

<objective>
Build opportunity aggregation from 80K Hours (via Algolia API) and aisafety.com (via Playwright scraping), with duplicate detection and scheduled daily sync.

Purpose: Automatically populate opportunities database from external sources (OPPS-03, OPPS-04), preventing cold start.

Output: Working scrapers that import opportunities on schedule, with duplicates merged and stale listings archived.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation-opportunities/CONTEXT.md
@.planning/phases/01-foundation-opportunities/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create 80K Hours Algolia adapter</name>
  <files>
    convex/aggregation/eightyK.ts
    package.json
  </files>
  <action>
1. Install Algolia client:
```bash
pnpm add algoliasearch
```

2. Create `convex/aggregation/eightyK.ts`:

```typescript
"use node";
import { action } from "../_generated/server";
import algoliasearch from "algoliasearch";

// 80K Hours uses Algolia for their job board search
// Credentials discovered from page source (public frontend keys)
// These may need updating if 80K Hours rotates their keys
const ALGOLIA_APP_ID = process.env.EIGHTY_K_ALGOLIA_APP_ID || "";
const ALGOLIA_API_KEY = process.env.EIGHTY_K_ALGOLIA_API_KEY || "";
const ALGOLIA_INDEX = "jobs_prod_super_ranked";

type AlgoliaHit = {
  objectID: string;
  title: string;
  company_name: string;
  location?: string;
  remote?: boolean;
  job_type?: string;
  experience_required?: string;
  description_short?: string;
  description?: string;
  requirements?: string[];
  salary_text?: string;
  closing_date?: string;
  posted_date?: string;
  url: string;
};

type NormalizedOpportunity = {
  sourceId: string;
  source: "80k_hours";
  title: string;
  organization: string;
  location: string;
  isRemote: boolean;
  roleType: string;
  experienceLevel?: string;
  description: string;
  requirements?: string[];
  salaryRange?: string;
  deadline?: number;
  sourceUrl: string;
  postedAt?: number;
};

export const fetchOpportunities = action({
  args: {},
  handler: async (): Promise<NormalizedOpportunity[]> => {
    if (!ALGOLIA_APP_ID || !ALGOLIA_API_KEY) {
      console.error("Missing 80K Hours Algolia credentials");
      return [];
    }

    const client = algoliasearch(ALGOLIA_APP_ID, ALGOLIA_API_KEY);

    const results: AlgoliaHit[] = [];
    let page = 0;
    let hasMore = true;

    try {
      while (hasMore) {
        const response = await client.searchSingleIndex({
          indexName: ALGOLIA_INDEX,
          searchParams: {
            query: "",
            page,
            hitsPerPage: 100,
          },
        });

        results.push(...(response.hits as AlgoliaHit[]));
        hasMore = page < (response.nbPages ?? 1) - 1;
        page++;

        // Rate limiting: 1 second between requests
        if (hasMore) {
          await new Promise((resolve) => setTimeout(resolve, 1000));
        }
      }

      console.log(`Fetched ${results.length} opportunities from 80K Hours`);
      return results.map(normalizeEightyKJob);
    } catch (error) {
      console.error("Error fetching from 80K Hours:", error);
      return [];
    }
  },
});

function normalizeEightyKJob(hit: AlgoliaHit): NormalizedOpportunity {
  return {
    sourceId: `80k-${hit.objectID}`,
    source: "80k_hours",
    title: hit.title || "Untitled",
    organization: hit.company_name || "Unknown Organization",
    location: hit.location || "Location not specified",
    isRemote: hit.remote ?? hit.location?.toLowerCase().includes("remote") ?? false,
    roleType: mapRoleType(hit.job_type),
    experienceLevel: mapExperienceLevel(hit.experience_required),
    description: hit.description || hit.description_short || "",
    requirements: hit.requirements,
    salaryRange: hit.salary_text,
    deadline: hit.closing_date ? new Date(hit.closing_date).getTime() : undefined,
    sourceUrl: hit.url,
    postedAt: hit.posted_date ? new Date(hit.posted_date).getTime() : undefined,
  };
}

function mapRoleType(jobType?: string): string {
  if (!jobType) return "other";
  const lower = jobType.toLowerCase();
  if (lower.includes("research")) return "research";
  if (lower.includes("engineer") || lower.includes("software") || lower.includes("technical"))
    return "engineering";
  if (lower.includes("operations") || lower.includes("ops") || lower.includes("admin"))
    return "operations";
  if (lower.includes("policy") || lower.includes("governance")) return "policy";
  return "other";
}

function mapExperienceLevel(exp?: string): string | undefined {
  if (!exp) return undefined;
  const lower = exp.toLowerCase();
  if (lower.includes("entry") || lower.includes("junior") || lower.includes("0-2"))
    return "entry";
  if (lower.includes("mid") || lower.includes("2-5") || lower.includes("3-5")) return "mid";
  if (lower.includes("senior") || lower.includes("5+") || lower.includes("5-10"))
    return "senior";
  if (lower.includes("lead") || lower.includes("principal") || lower.includes("director"))
    return "lead";
  return undefined;
}
```

3. Add environment variables to `.env.local`:
```
# 80K Hours Algolia (extract from 80000hours.org/jobs page source)
EIGHTY_K_ALGOLIA_APP_ID=
EIGHTY_K_ALGOLIA_API_KEY=
```

NOTE: The Algolia credentials need to be extracted from the 80K Hours website page source. Visit https://80000hours.org/job-board/ and view source to find the Algolia config. These are public frontend keys.
  </action>
  <verify>
File exists: `convex/aggregation/eightyK.ts`
`pnpm convex dev` shows function deployed without errors.
  </verify>
  <done>80K Hours adapter ready (will need credentials populated to actually fetch data).</done>
</task>

<task type="auto">
  <name>Task 2: Create aisafety.com Playwright scraper</name>
  <files>
    convex/aggregation/aisafety.ts
    package.json
  </files>
  <action>
1. Install Playwright:
```bash
pnpm add playwright
pnpm exec playwright install chromium
```

2. Create `convex/aggregation/aisafety.ts`:

```typescript
"use node";
import { action } from "../_generated/server";
import { chromium } from "playwright";
import crypto from "crypto";

type ScrapedJob = {
  title: string;
  organization: string;
  location: string;
  isRemote: boolean;
  sourceUrl: string;
  description?: string;
};

type NormalizedOpportunity = {
  sourceId: string;
  source: "aisafety_com";
  title: string;
  organization: string;
  location: string;
  isRemote: boolean;
  roleType: string;
  description: string;
  sourceUrl: string;
};

export const fetchOpportunities = action({
  args: {},
  handler: async (): Promise<NormalizedOpportunity[]> => {
    let browser;
    try {
      browser = await chromium.launch({ headless: true });
      const page = await browser.newPage();

      // Set realistic user agent
      await page.setExtraHTTPHeaders({
        "User-Agent":
          "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
      });

      await page.goto("https://www.aisafety.com/jobs", {
        waitUntil: "networkidle",
        timeout: 30000,
      });

      // Wait for content to load (Webflow + Finsweet CMS)
      // Try multiple possible selectors since Webflow sites vary
      const possibleSelectors = [
        ".featured-card",
        ".job-card",
        ".w-dyn-item",
        "[data-w-id]",
        ".job-listing",
      ];

      let contentLoaded = false;
      for (const selector of possibleSelectors) {
        try {
          await page.waitForSelector(selector, { timeout: 5000 });
          contentLoaded = true;
          console.log(`Found content using selector: ${selector}`);
          break;
        } catch {
          continue;
        }
      }

      if (!contentLoaded) {
        console.warn("Could not find job cards on aisafety.com - DOM may have changed");
        await browser.close();
        return [];
      }

      const opportunities = await page.evaluate(() => {
        // Try multiple selector patterns
        const cardSelectors = [
          ".featured-card",
          ".job-card",
          ".w-dyn-item",
          "[class*='job']",
          "[class*='listing']",
        ];

        let cards: Element[] = [];
        for (const selector of cardSelectors) {
          const found = document.querySelectorAll(selector);
          if (found.length > 0) {
            cards = Array.from(found);
            break;
          }
        }

        return cards.map((card) => {
          // Try multiple patterns for each field
          const titleEl =
            card.querySelector("h3") ||
            card.querySelector("h2") ||
            card.querySelector("[class*='title']") ||
            card.querySelector("[class*='name']");

          const orgEl =
            card.querySelector("[class*='organization']") ||
            card.querySelector("[class*='company']") ||
            card.querySelector("[class*='org']");

          const locationEl =
            card.querySelector("[class*='location']") ||
            card.querySelector("[class*='place']");

          const link =
            card.querySelector("a[href*='job']") ||
            card.querySelector("a[href*='career']") ||
            card.querySelector("a");

          const title = titleEl?.textContent?.trim() || "";
          const organization = orgEl?.textContent?.trim() || "";
          const location = locationEl?.textContent?.trim() || "Remote";

          return {
            title,
            organization,
            location,
            isRemote:
              location.toLowerCase().includes("remote") ||
              location.toLowerCase().includes("anywhere"),
            sourceUrl: link?.getAttribute("href") || "",
            description: "", // Would need to scrape detail pages for full description
          };
        });
      });

      await browser.close();

      // Filter out invalid entries
      const validOpportunities = opportunities.filter(
        (opp) => opp.title && opp.title.length > 0
      );

      console.log(`Scraped ${validOpportunities.length} opportunities from aisafety.com`);

      return validOpportunities.map((opp) => ({
        sourceId: `aisafety-${hashString(opp.title + opp.organization)}`,
        source: "aisafety_com" as const,
        title: opp.title,
        organization: opp.organization || "Unknown",
        location: opp.location,
        isRemote: opp.isRemote,
        roleType: inferRoleType(opp.title),
        description: opp.description || `View full details at ${opp.sourceUrl}`,
        sourceUrl: opp.sourceUrl.startsWith("http")
          ? opp.sourceUrl
          : `https://www.aisafety.com${opp.sourceUrl}`,
      }));
    } catch (error) {
      console.error("Error scraping aisafety.com:", error);
      if (browser) await browser.close();
      return [];
    }
  },
});

function hashString(str: string): string {
  return crypto.createHash("md5").update(str).digest("hex").substring(0, 12);
}

function inferRoleType(title: string): string {
  const lower = title.toLowerCase();
  if (lower.includes("research")) return "research";
  if (
    lower.includes("engineer") ||
    lower.includes("developer") ||
    lower.includes("software")
  )
    return "engineering";
  if (
    lower.includes("operations") ||
    lower.includes("ops") ||
    lower.includes("coordinator")
  )
    return "operations";
  if (lower.includes("policy") || lower.includes("governance")) return "policy";
  return "other";
}
```

NOTE: The scraper uses resilient selectors that try multiple patterns. Webflow sites can change DOM structure frequently. If scraping fails, inspect aisafety.com/jobs to identify current selectors.
  </action>
  <verify>
File exists: `convex/aggregation/aisafety.ts`
`pnpm convex dev` shows function deployed without errors.
  </verify>
  <done>aisafety.com scraper ready with resilient selector patterns.</done>
</task>

<task type="auto">
  <name>Task 3: Create sync orchestration with deduplication</name>
  <files>
    convex/aggregation/dedup.ts
    convex/aggregation/sync.ts
    package.json
  </files>
  <action>
1. Install string-similarity:
```bash
pnpm add string-similarity
pnpm add -D @types/string-similarity
```

2. Create `convex/aggregation/dedup.ts`:

```typescript
import stringSimilarity from "string-similarity";

export function normalizeTitle(title: string): string {
  return title
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, "")
    .replace(/\s+/g, " ")
    .trim();
}

export function normalizeOrganization(org: string): string {
  return org
    .toLowerCase()
    .replace(/,?\s*(inc|llc|ltd|pbc|corp|corporation)\.?$/i, "")
    .replace(/[^a-z0-9\s]/g, "")
    .replace(/\s+/g, " ")
    .trim();
}

export function isSimilarOpportunity(
  a: { title: string; organization: string },
  b: { title: string; organization: string },
  threshold = 0.85
): boolean {
  const titleSimilarity = stringSimilarity.compareTwoStrings(
    normalizeTitle(a.title),
    normalizeTitle(b.title)
  );

  const orgSimilarity = stringSimilarity.compareTwoStrings(
    normalizeOrganization(a.organization),
    normalizeOrganization(b.organization)
  );

  // Both title and org must be similar
  return titleSimilarity > threshold && orgSimilarity > 0.8;
}
```

3. Create `convex/aggregation/sync.ts`:

```typescript
"use node";
import { internalAction, internalMutation } from "../_generated/server";
import { internal } from "../_generated/api";
import { v } from "convex/values";
import { isSimilarOpportunity } from "./dedup";

const opportunityValidator = v.object({
  sourceId: v.string(),
  source: v.union(
    v.literal("80k_hours"),
    v.literal("aisafety_com"),
    v.literal("manual")
  ),
  title: v.string(),
  organization: v.string(),
  location: v.string(),
  isRemote: v.boolean(),
  roleType: v.string(),
  experienceLevel: v.optional(v.string()),
  description: v.string(),
  requirements: v.optional(v.array(v.string())),
  salaryRange: v.optional(v.string()),
  deadline: v.optional(v.number()),
  sourceUrl: v.string(),
  postedAt: v.optional(v.number()),
});

export const runFullSync = internalAction({
  args: {},
  handler: async (ctx) => {
    console.log("Starting full opportunity sync...");

    // Fetch from both sources in parallel
    const [eightyKJobs, aisafetyJobs] = await Promise.all([
      ctx.runAction(internal.aggregation.eightyK.fetchOpportunities, {}),
      ctx.runAction(internal.aggregation.aisafety.fetchOpportunities, {}),
    ]);

    console.log(
      `Fetched: ${eightyKJobs.length} from 80K Hours, ${aisafetyJobs.length} from aisafety.com`
    );

    // Combine all jobs
    const allJobs = [...eightyKJobs, ...aisafetyJobs];

    if (allJobs.length === 0) {
      console.log("No opportunities fetched from any source");
      return;
    }

    // Upsert opportunities with deduplication
    await ctx.runMutation(internal.aggregation.sync.upsertOpportunities, {
      opportunities: allJobs,
    });

    // Archive opportunities that disappeared from sources
    await ctx.runMutation(internal.aggregation.sync.archiveMissing, {
      currentSourceIds: allJobs.map((j) => j.sourceId),
    });

    console.log("Sync complete");
  },
});

export const upsertOpportunities = internalMutation({
  args: {
    opportunities: v.array(opportunityValidator),
  },
  handler: async (ctx, args) => {
    let inserted = 0;
    let updated = 0;
    let merged = 0;

    for (const opp of args.opportunities) {
      // Check for exact source match first
      const existing = await ctx.db
        .query("opportunities")
        .withIndex("by_source_id", (q) => q.eq("sourceId", opp.sourceId))
        .unique();

      if (existing) {
        // Update existing opportunity
        await ctx.db.patch(existing._id, {
          title: opp.title,
          organization: opp.organization,
          location: opp.location,
          isRemote: opp.isRemote,
          roleType: opp.roleType,
          experienceLevel: opp.experienceLevel,
          description: opp.description,
          requirements: opp.requirements,
          salaryRange: opp.salaryRange,
          deadline: opp.deadline,
          sourceUrl: opp.sourceUrl,
          lastVerified: Date.now(),
          updatedAt: Date.now(),
        });
        updated++;
      } else {
        // Check for fuzzy duplicate from different source
        const sameOrg = await ctx.db
          .query("opportunities")
          .withIndex("by_organization", (q) => q.eq("organization", opp.organization))
          .collect();

        const duplicate = sameOrg.find((existing) =>
          isSimilarOpportunity(
            { title: existing.title, organization: existing.organization },
            { title: opp.title, organization: opp.organization }
          )
        );

        if (duplicate) {
          // Add as alternate source
          const alternateSources = duplicate.alternateSources || [];
          const alreadyListed = alternateSources.some(
            (s) => s.sourceId === opp.sourceId
          );

          if (!alreadyListed) {
            await ctx.db.patch(duplicate._id, {
              alternateSources: [
                ...alternateSources,
                {
                  sourceId: opp.sourceId,
                  source: opp.source,
                  sourceUrl: opp.sourceUrl,
                },
              ],
              lastVerified: Date.now(),
              updatedAt: Date.now(),
            });
            merged++;
          }
        } else {
          // Insert new opportunity
          await ctx.db.insert("opportunities", {
            sourceId: opp.sourceId,
            source: opp.source,
            title: opp.title,
            organization: opp.organization,
            location: opp.location,
            isRemote: opp.isRemote,
            roleType: opp.roleType,
            experienceLevel: opp.experienceLevel,
            description: opp.description,
            requirements: opp.requirements,
            salaryRange: opp.salaryRange,
            deadline: opp.deadline,
            sourceUrl: opp.sourceUrl,
            status: "active",
            lastVerified: Date.now(),
            createdAt: Date.now(),
            updatedAt: Date.now(),
          });
          inserted++;
        }
      }
    }

    console.log(
      `Upsert complete: ${inserted} inserted, ${updated} updated, ${merged} merged`
    );
  },
});

export const archiveMissing = internalMutation({
  args: {
    currentSourceIds: v.array(v.string()),
  },
  handler: async (ctx, args) => {
    const sourceIdSet = new Set(args.currentSourceIds);

    // Get all active non-manual opportunities
    const activeOpportunities = await ctx.db
      .query("opportunities")
      .withIndex("by_status", (q) => q.eq("status", "active"))
      .collect();

    let archived = 0;
    for (const opp of activeOpportunities) {
      // Don't archive manual entries or opportunities still in source
      if (opp.source === "manual" || sourceIdSet.has(opp.sourceId)) {
        continue;
      }

      // Also check if any alternate source is still active
      const hasActiveAlternate = opp.alternateSources?.some((alt) =>
        sourceIdSet.has(alt.sourceId)
      );

      if (!hasActiveAlternate) {
        await ctx.db.patch(opp._id, {
          status: "archived",
          updatedAt: Date.now(),
        });
        archived++;
      }
    }

    if (archived > 0) {
      console.log(`Archived ${archived} opportunities no longer in sources`);
    }
  },
});

// Manual trigger for testing
export const triggerSync = internalAction({
  args: {},
  handler: async (ctx) => {
    await ctx.runAction(internal.aggregation.sync.runFullSync, {});
  },
});
```
  </action>
  <verify>
Files exist: `convex/aggregation/dedup.ts`, `convex/aggregation/sync.ts`
`pnpm convex dev` shows all functions deployed without errors.
  </verify>
  <done>Sync orchestration with deduplication and archiving logic complete.</done>
</task>

<task type="auto">
  <name>Task 4: Set up cron job for daily sync</name>
  <files>
    convex/crons.ts
  </files>
  <action>
Create `convex/crons.ts`:

```typescript
import { cronJobs } from "convex/server";
import { internal } from "./_generated/api";

const crons = cronJobs();

// Run daily at 6 AM UTC
// This syncs opportunities from 80K Hours and aisafety.com
crons.daily(
  "sync-opportunities",
  { hourUTC: 6, minuteUTC: 0 },
  internal.aggregation.sync.runFullSync
);

export default crons;
```

After creating the file, verify cron is registered:
- Run `pnpm convex dev`
- Check Convex dashboard -> Functions -> Crons tab
- Should show "sync-opportunities" scheduled for 6:00 UTC daily

To manually trigger sync for testing:
1. Go to Convex dashboard
2. Functions tab
3. Find `aggregation.sync.triggerSync`
4. Click "Run" to execute manually

NOTE: For first run, you'll need to populate the 80K Hours Algolia credentials. Visit https://80000hours.org/job-board/, view page source, search for "algolia" to find the app ID and API key.
  </action>
  <verify>
File exists: `convex/crons.ts`
Convex dashboard shows cron job registered.
  </verify>
  <done>Daily sync cron configured at 6 AM UTC.</done>
</task>

</tasks>

<verification>
Verify plan completion:

```bash
# All files exist
ls convex/aggregation/

# Convex shows functions
pnpm convex dev

# Check dashboard:
# - Functions tab shows eightyK.fetchOpportunities, aisafety.fetchOpportunities, sync.runFullSync
# - Crons tab shows sync-opportunities scheduled
```

Manual test (after adding Algolia credentials):
1. Go to Convex dashboard
2. Run `aggregation.sync.triggerSync` action
3. Check Data tab - opportunities should appear
4. Run again - should update existing, not duplicate
</verification>

<success_criteria>
- 80K Hours adapter queries Algolia API (OPPS-03)
- aisafety.com scraper extracts job listings (OPPS-04)
- Duplicate opportunities from different sources are merged
- Opportunities removed from source are auto-archived
- Cron job runs daily at 6 AM UTC
- lastVerified updated on each sync (supports OPPS-06)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-opportunities/01-03-SUMMARY.md`
</output>
