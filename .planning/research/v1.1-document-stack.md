# Stack Research: Document Parsing + LLM Extraction (v1.1)

**Project:** ASTN v1.1 - Document Upload Feature
**Researched:** 2026-01-18
**Overall Confidence:** HIGH

## Executive Summary

Adding PDF/document upload to ASTN requires three components: (1) file upload/storage in Convex, (2) PDF text extraction in Node.js, and (3) LLM-powered structured data extraction using Claude. The existing codebase already demonstrates the extraction pattern using Claude's `tool_choice` feature in `convex/enrichment/extraction.ts` - the v1.1 feature extends this to work with document text instead of conversation history.

---

## Recommended Stack

### File Upload & Storage

| Technology | Version | Purpose | Why |
|------------|---------|---------|-----|
| Convex File Storage | Built-in | Store uploaded PDFs | Native Convex feature, no additional dependencies. Integrates with existing auth and storage billing. |
| `generateUploadUrl()` | Built-in | Client-side upload flow | Recommended pattern for files. Upload URL valid for 1 hour. |

**How it works:**
1. Client calls mutation to get upload URL via `ctx.storage.generateUploadUrl()`
2. Client POSTs file directly to that URL (max 2-minute timeout)
3. Client receives `storageId` and passes to another mutation to save metadata
4. In action, retrieve file via `ctx.storage.get(storageId)` as a Blob

**File Size Limits:**
- HTTP Actions: 20MB hard limit
- Upload URLs: No explicit limit (constrained by 2-minute upload timeout)
- For CVs/LinkedIn PDFs: Typical size 50KB-2MB, well within limits

### PDF Text Extraction

| Library | Version | Purpose | Why |
|---------|---------|---------|-----|
| **unpdf** | ^0.12.x | Primary PDF parser | Modern, maintained wrapper around pdf.js. Cross-platform (works in Convex actions). No native dependencies. Part of UnJS ecosystem. |
| pdfjs-dist | (peer dep) | PDF rendering engine | Mozilla's battle-tested PDF library. unpdf abstracts the complexity. |

**Installation:**
```bash
npm install unpdf pdfjs-dist
```

**Basic usage:**
```typescript
"use node";
import { extractText } from "unpdf";

export const parseDocument = action({
  args: { storageId: v.id("_storage") },
  handler: async (ctx, { storageId }) => {
    const blob = await ctx.storage.get(storageId);
    if (!blob) throw new Error("File not found");

    const buffer = await blob.arrayBuffer();
    const { text } = await extractText(new Uint8Array(buffer));
    return text;
  },
});
```

### LLM Extraction

| Technology | Version | Purpose | Why |
|------------|---------|---------|-----|
| @anthropic-ai/sdk | ^0.71.x | Claude API client | Already in use (`package.json`). Provides TypeScript types for tools. |
| Claude Haiku 4.5 | claude-haiku-4-5-20251001 | Extraction model | Fast, cheap, sufficient for structured extraction. Already used in enrichment. |
| `tool_choice: { type: "tool" }` | API feature | Force structured output | Guarantees JSON output matching schema. Already proven in `extraction.ts`. |

**Existing pattern in codebase (verified - HIGH confidence):**
```typescript
// From convex/enrichment/extraction.ts
const response = await anthropic.messages.create({
  model: "claude-haiku-4-5-20251001",
  max_tokens: 1024,
  tools: [profileExtractionTool],
  tool_choice: { type: "tool", name: "extract_profile_info" },
  system: "...",
  messages: [...],
});

const toolUse = response.content.find((block) => block.type === "tool_use");
return toolUse.input as ExtractionResult;
```

---

## PDF Parsing Options Comparison

| Library | Pros | Cons | Recommendation |
|---------|------|------|----------------|
| **unpdf** | Modern API, no native deps, TypeScript, maintained (UnJS) | Newer (less battle-tested) | **USE THIS** |
| pdf-parse | Popular, simple API | Unmaintained (2019), test file pollution, suspicious npm code | **AVOID** |
| pdf.js-extract | Direct pdf.js wrapper | More verbose API, less maintained | Alternative if unpdf fails |
| pdfjs-dist (direct) | Most control, Mozilla-backed | Complex API, more boilerplate | Only if you need fine control |
| pdf2json | Full metadata extraction | Heavier, more complex output | Overkill for text extraction |
| @papra/lecture | Simple extractText API | Less popular, fewer downloads | Alternative option |

**Decision: Use unpdf** because:
1. Simplest API for text extraction (`extractText(buffer)`)
2. No native dependencies (critical for Convex Node.js runtime)
3. Actively maintained as part of UnJS ecosystem (same maintainers as nuxt, h3, ofetch)
4. TypeScript types included

---

## What NOT to Use

### PDF Parsing - Avoid These

| Library | Why Avoid |
|---------|-----------|
| **pdf-parse** | Unmaintained since 2019, has known issues with test files leaking into npm package, suspicious test code |
| **pdf2pic** / **node-poppler** | Requires native Poppler installation, won't work in Convex runtime |
| **Apache Tika** | Requires Java, overkill for simple text extraction |
| **pdfreader** | Has `userAgent` undefined error in some Node versions |

### LLM Patterns - Avoid These

| Pattern | Why Avoid |
|---------|-----------|
| **JSON mode without tools** | Less reliable structure guarantee, no schema validation |
| **String parsing of LLM output** | Brittle, breaks on formatting changes |
| **Claude Sonnet for extraction** | Overkill cost/latency for structured extraction - Haiku is sufficient |
| **Multiple extraction calls** | Combine into single tool with all fields for efficiency |
| **output_format beta** | Still in beta (anthropic-beta header required), tool_choice is stable |

### Storage - Avoid These

| Approach | Why Avoid |
|---------|-----------|
| **HTTP Actions for upload** | 20MB limit, more complex CORS handling |
| **External storage (S3, R2)** | Unnecessary complexity, Convex storage is included |
| **Base64 in database** | Bloats database, Convex file storage is designed for this |

---

## Integration with Existing Codebase

### Already Have (no new dependencies needed for LLM)

From `package.json`:
- `@anthropic-ai/sdk: ^0.71.2` - Claude API client
- `convex: ^1.31.0` - Backend with file storage

From `convex/enrichment/extraction.ts`:
- Tool definition pattern for structured extraction
- `tool_choice: { type: "tool", name: "..." }` pattern
- Haiku model usage for fast extraction

### New Dependencies Needed

```bash
# PDF parsing (add to package.json)
npm install unpdf pdfjs-dist
```

That's it - one new library with its peer dependency.

---

## Implementation Pattern

### Document Extraction Tool Schema

Extends existing pattern from `convex/enrichment/extraction.ts`:

```typescript
// convex/documents/extraction.ts
"use node";

import { v } from "convex/values";
import Anthropic from "@anthropic-ai/sdk";
import { extractText } from "unpdf";
import { action } from "../_generated/server";

const documentExtractionTool: Anthropic.Tool = {
  name: "extract_profile_from_document",
  description: "Extract structured profile information from a CV or resume document",
  input_schema: {
    type: "object" as const,
    properties: {
      // Basic info
      name: { type: "string", description: "Full name of the person" },
      location: { type: "string", description: "Current location/city" },
      headline: { type: "string", description: "Professional headline or title" },

      // Education (matches schema.ts structure)
      education: {
        type: "array",
        items: {
          type: "object",
          properties: {
            institution: { type: "string" },
            degree: { type: "string" },
            field: { type: "string" },
            startYear: { type: "number" },
            endYear: { type: "number" },
          },
          required: ["institution"],
        },
      },

      // Work history (matches schema.ts structure)
      workHistory: {
        type: "array",
        items: {
          type: "object",
          properties: {
            organization: { type: "string" },
            title: { type: "string" },
            startDate: { type: "string" }, // ISO format, convert to timestamp
            endDate: { type: "string" },
            current: { type: "boolean" },
            description: { type: "string" },
          },
          required: ["organization", "title"],
        },
      },

      // Skills
      skills: {
        type: "array",
        items: { type: "string" },
        description: "Technical and professional skills",
      },

      // AI Safety specific
      aiSafetyInterests: {
        type: "array",
        items: { type: "string" },
        description: "AI safety topics or research areas mentioned",
      },
      careerGoals: {
        type: "string",
        description: "Career aspirations mentioned in the document",
      },
    },
    required: ["name"],
  },
};

export const extractFromDocument = action({
  args: { storageId: v.id("_storage") },
  handler: async (ctx, { storageId }) => {
    // 1. Get file from storage
    const blob = await ctx.storage.get(storageId);
    if (!blob) throw new Error("File not found");

    // 2. Extract text from PDF
    const buffer = await blob.arrayBuffer();
    const { text } = await extractText(new Uint8Array(buffer));

    // 3. Truncate if too long (Haiku context is 200K, but keep it reasonable)
    const truncatedText = text.slice(0, 50000); // ~12k tokens

    // 4. Extract structured data via Claude tool use
    const anthropic = new Anthropic();
    const response = await anthropic.messages.create({
      model: "claude-haiku-4-5-20251001",
      max_tokens: 2048,
      tools: [documentExtractionTool],
      tool_choice: { type: "tool", name: "extract_profile_from_document" },
      system: `You are extracting structured profile information from a CV/resume for an AI safety career platform.
Extract all relevant professional information. Be thorough but only include information actually present.
For dates, use ISO format (YYYY-MM-DD) or just year (YYYY) if month unknown.
For skills, include both technical skills and domain expertise.`,
      messages: [{ role: "user", content: truncatedText }],
    });

    // 5. Extract tool result
    const toolUse = response.content.find((block) => block.type === "tool_use");
    if (!toolUse || toolUse.type !== "tool_use") {
      throw new Error("Extraction failed - no tool use in response");
    }

    return toolUse.input;
  },
});
```

---

## Token/Cost Estimation

| Document | Typical Size | Tokens | Haiku Cost |
|----------|--------------|--------|------------|
| 2-page CV | 2-4KB text | ~800-1600 | ~$0.0004 |
| LinkedIn PDF | 5-10KB text | ~2000-4000 | ~$0.001 |
| Max (50KB) | 50KB text | ~12000 | ~$0.003 |

Cost is negligible even at scale. At 1000 extractions/month: ~$3.

---

## Supported File Types

| Type | Content-Type | Handling |
|------|--------------|----------|
| PDF | application/pdf | unpdf extraction |
| Plain text | text/plain | Direct use (text paste path) |

Future consideration: LinkedIn exports (PDF with specific formatting).

---

## Confidence Assessment

| Area | Level | Reason |
|------|-------|--------|
| File Upload | HIGH | Convex docs verified, standard pattern |
| unpdf library | MEDIUM | UnJS ecosystem is reputable, but less battle-tested than pdf.js directly |
| Claude extraction | HIGH | Already working in codebase (`extraction.ts`) |
| Tool choice pattern | HIGH | Verified in existing code |

---

## Sources

**HIGH confidence (verified):**
- Existing codebase: `convex/enrichment/extraction.ts` - tool_choice pattern
- Existing codebase: `package.json` - @anthropic-ai/sdk ^0.71.2
- Convex File Storage: https://docs.convex.dev/file-storage/upload-files
- Claude Tool Use: https://platform.claude.com/cookbook/tool-use-extracting-structured-json

**MEDIUM confidence (npm, GitHub):**
- unpdf: https://github.com/unjs/unpdf
- pdf.js-extract: https://github.com/ffalt/pdf.js-extract

**Research notes:**
- pdf-parse investigated and rejected (unmaintained, security concerns)
- Multiple PDF libraries compared via npm download stats and GitHub activity
